{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04, Feb 11 2020.\n",
    "### Due Feb 18, 2020, before lab.\n",
    "### Building your own COMPAS like predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will build your own COMPAS like predictor with the data we have been using for the last two labs. To do this, we will use logistic regression. To recall logistic regression, refer to [videos on Logistic Regression posted on the schedule for today](https://www.youtube.com/watch?v=vN5cNN2-HWE) and to [Lab 1](https://render.githubusercontent.com/view/ipynb?commit=06e777dc2ca6fea2d8bd7e98919ac8dc8847c051&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4275636b6e656c6c466169726e6573732f435343493337392d535032302f303665373737646332636136666561326438626437653938393139616338646338383437633035312f4c6162732f6c6162312f6c6162312e6970796e62&nwo=BucknellFairness%2FCSCI379-SP20&path=Labs%2Flab1%2Flab1.ipynb&repository_id=233859195&repository_type=Repository#Onto-Classification) on how to use it in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Getting Started\n",
    "To get started, like before, create `lab4` folder in your local repo alongwith a `data` folder. In the data folder, place the data file we have been using for labs 2 and 3. (The csv file can be downloaded from [here](https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv)). Place it in your `data` folder for `lab4`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure we have all the right libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "import pylab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('data/compas-scores-two-years.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will filter out rows where **days_b_screening_arrest** is over 30 or under -30, leaving us with 6,172 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out data\n",
    "filterData = data[(data['days_b_screening_arrest'] <= 30) & (data['days_b_screening_arrest'] >= -30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the data is the correct size\n",
    "filterData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are going to use logistic regression to try to build our own predictor. To begin with, we will use the following variables.\n",
    "\n",
    "* Age\n",
    "* Sex\n",
    "* Felony or Misdemeanor charge (`c_charge_degree`)\n",
    "* Number of prior arrests (`priors_count`)\n",
    "\n",
    " We will also try this both with and without race as a predictive factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you haven't seen this before, here is a quick way of getting all the column names for your Pandas dataframe. As you can see there are 45+ features in this data frame, but we will find that only a few are enough to achieve an accuracy comparable to COMPAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way of getting a quick look at the data by an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterData.sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **crosstab** function in pandas is a useful function to familiarize yourself with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recidivsm rates by race\n",
    "recid_by_race = pd.crosstab(filterData.race, filterData.two_year_recid)\n",
    "recid_by_race "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can insert another column in this dataframe called \"rate\"\n",
    "recid_by_race['rate'] = recid_by_race[1]/recid_by_race.sum(axis = 1)\n",
    "recid_by_race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1** Now create a similar dataframe to view recidivism rates by sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you move further along, answer the following questions. Inspect the output of `filterData.columns()` to answer the first two. \n",
    "\n",
    "* Q1 Which variable in the COMPAS data represents the outcomes of the predictor built by Northpointe?\n",
    "* Q2: Which variable represents the \"reality\"?\n",
    "* Q3: What is a confusion matrix and why is it useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `crosstab` function to derive the confusion matrix for a predictor. In this case, the one used by ProPublica that we have seen in our prior labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_recid = filterData['score_text']!= 'Low'\n",
    "actual_recid = filterData.two_year_recid == 1\n",
    "conf_matrix = pd.crosstab(actual_recid,guessed_recid, rownames = ['actual_recid'], colnames = ['guessed_recid'])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Using logistic regression:\n",
    "First, we will convert the `age`, `sex` and `c_charge_degree` (which are all categorical data) to what are called indicator variables. Make sure you read [this resource on logistic regression and so called dummy variables](https://www.statisticssolutions.com/dummy-coding-the-how-and-why/) before going any further.\n",
    "\n",
    " The code below uses the pandas function `get_dummies` to convert `age`, `sex`, and `charge_degree` to dummy variables, and concatenates the resulting data into a new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat(\n",
    "    [pd.get_dummies(filterData.age_cat, prefix='age'),\n",
    "     pd.get_dummies(filterData.sex, prefix='sex'),\n",
    "     pd.get_dummies(filterData.c_charge_degree, prefix='degree'), # felony or misdemeanor charge ('f' or 'm')\n",
    "     filterData.priors_count],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will drop one of the [dummy variables](https://en.wikiversity.org/wiki/Dummy_variable_(statistics)) for each category because they are redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop(['age_25 - 45', 'sex_Female', 'degree_M'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true target variable we're trying\n",
    "# to predict: whether someone is re-arrested.\n",
    "target = filterData.two_year_recid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the values in the features \n",
    "x = features.values\n",
    "# the values in the target\n",
    "y = target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing what we need\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the LR model to x. \n",
    "lr.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to understand what the coefficients of the LR model mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "coeffs = pd.DataFrame(np.exp(lr.coef_), columns=features.columns)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model thinks that:\n",
    "\n",
    "* being young (<25) more than doubles your odds of recidivism\n",
    "* but being >45 years old makes half as likely\n",
    "* being male increases your odds by 40%\n",
    "* every prior arrest increases your odds by 18%\n",
    "\n",
    "Q: Explain why that's the case? You might want to review the [video on coefficients in logistic regression](https://www.youtube.com/watch?v=vN5cNN2-HWE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2** Now make a prediction according to the LR model, and compare it to the actual target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume y_pred has the prediction from the LR model.\n",
    "y_pred =  # fill in your answer here. In the code below\n",
    "          # I'm assuming that y_pred is a numpy array\n",
    "\n",
    "# this code converts the y_pred numpy array into a Pandas series\n",
    "guessed=pd.Series(y_pred, index=features.index)==1\n",
    "\n",
    "# This is the target variable, also as a series.\n",
    "actual=filterData.two_year_recid==1\n",
    "\n",
    "# The confusion matrix for the guessed and actual prediction\n",
    "cm = pd.crosstab(guessed, actual, rownames=['guessed'], colnames=['actual'])\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has helper functions for us to print the confusion matrix and the accuracy rates for a predictor. It will be of use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm is a confusion matrix. The rows are guessed, the columns are actual \n",
    "def print_ppv_fpv(cm):\n",
    "    # takes in a confusion matrix\n",
    "    # the indices here are [col][row] or [actual][guessed]\n",
    "    # prints the various metrics\n",
    "    TN = cm[False][False]   \n",
    "    TP = cm[True][True]\n",
    "    FN = cm[True][False]\n",
    "    FP = cm[False][True]\n",
    "    print('Accuracy: ', (TN+TP)/(TN+TP+FN+FP))\n",
    "    print('PPV: ', TP / (TP + FP))\n",
    "    print('FPR: ', FP / (FP + TN))\n",
    "    print('FNR: ', FN / (FN + TP))\n",
    "    print()\n",
    "\n",
    "def print_metrics(guessed, actual):\n",
    "    # takes in series of guessed and actual predictions\n",
    "    cm = pd.crosstab(guessed, actual, rownames=['guessed'], colnames=['actual'])\n",
    "    print(cm)\n",
    "    print()\n",
    "    print_ppv_fpv(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the accuracy of our LR model.\n",
    "print('Our Logistic Regression for the enire group')\n",
    "print_metrics(guessed, actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3** Now use the **print_metrics** function to print the Confusion Matrix and all the rates for both the Black and Caucasian Group according to our predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Task 3\n",
    "print('White')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4** How does this compare to the rates for the COMPAS predictor from Labs 2 and 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Task 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5** Now, repeat the entire prediction process by including race in the feature variables. How do the different rates compare to the case when race was not explicitly a feature variable? Do you have an explanation for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to Task 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6** Refer to Lab 1 to plot an ROC curve for the first LR predictor that does *not* include race as a feature. Plot the ROC curves for the entire population as well as the ROC curves for the the 'African-American' and 'Caucasian' group. \n",
    "\n",
    "Plot another figure that shows the ROC curves when race is explicitly used as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7** Now, we will use 7 features to train our logistic regression classifier - age, sex, number of juvenile misdemeanors, number of juvenile felonies, number of prior (nonjuvenile) crimes, crime degree, and crime charge. How does this compare to our first classifier, that used only four features? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
