{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05, Feb 18 2020.\n",
    "### Due Feb 23, 2020, before lab.\n",
    "### Building your own COMPAS like predictor - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will build your own COMPAS like predictor with the data we have been using for the last two labs. To do this, we will use logistic regression. To recall logistic regression, refer to [videos on Logistic Regression posted on the schedule for today](https://www.youtube.com/watch?v=vN5cNN2-HWE) and to [Lab 1](https://render.githubusercontent.com/view/ipynb?commit=06e777dc2ca6fea2d8bd7e98919ac8dc8847c051&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4275636b6e656c6c466169726e6573732f435343493337392d535032302f303665373737646332636136666561326438626437653938393139616338646338383437633035312f4c6162732f6c6162312f6c6162312e6970796e62&nwo=BucknellFairness%2FCSCI379-SP20&path=Labs%2Flab1%2Flab1.ipynb&repository_id=233859195&repository_type=Repository#Onto-Classification) on how to use it in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "To get started, like before, create `lab5` folder in your local repo alongwith a `data` folder. In the data folder, place the data file we have been using for labs 2 and 3. (The csv file can be downloaded from [here](https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv)). Place it in your `data` folder for `lab5`. \n",
    "\n",
    "Refer to your `lab4` submission to continue with this. You may use a copy of that notebook to complete these tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure we have all the right libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "import pylab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('data/compas-scores-two-years.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will filter out rows where **days_b_screening_arrest** is over 30 or under -30, leaving us with 6,172 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out data\n",
    "filterData = data[(data['days_b_screening_arrest'] <= 30) & (data['days_b_screening_arrest'] >= -30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the data is the correct size\n",
    "filterData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are going to use logistic regression to try to build our own predictor. To begin with, we will use the following variables.\n",
    "\n",
    "* Age\n",
    "* Sex\n",
    "* Felony or Misdemeanor charge (`c_charge_degree`)\n",
    "* Number of prior arrests (`priors_count`)\n",
    "\n",
    " We will also try this both with and without race as a predictive factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you haven't seen this before, here is a quick way of getting all the column names for your Pandas dataframe. As you can see there are 45+ features in this data frame, but we will find that only a few are enough to achieve an accuracy comparable to COMPAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way of getting a quick look at the data by an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterData.sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **crosstab** function in pandas is a useful function to familiarize yourself with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recidivsm rates by race\n",
    "recid_by_race = pd.crosstab(filterData.race, filterData.two_year_recid)\n",
    "recid_by_race "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can insert another column in this dataframe called \"rate\"\n",
    "recid_by_race['rate'] = recid_by_race[1]/recid_by_race.sum(axis = 1)\n",
    "recid_by_race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### **Task 1** \n",
    " Refer to Lab 1 to plot an ROC curve for the LR predictor from Lab 4 that does *not* include race as a feature. Plot the ROC curves for the entire population as well as the ROC curves for the the 'African-American' and 'Caucasian' group. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 \n",
    "\n",
    "Plot another figure that shows the ROC curves when race is explicitly used as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3** \n",
    "Now, use 7 features to train our logistic regression classifier - age, sex, number of juvenile misdemeanors, number of juvenile felonies, number of prior (nonjuvenile) crimes, crime degree, and crime charge. You will need to redo all the work from Part 2 of Lab 4 for seven of these variables, including creating the dummy variables, dropping the redundant variable, fitting the data to the model, and evaluating how good the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task4**\n",
    "How does this compare to our first classifier in Lab 4, that used only four features? Use the `print_metrics` function from Lab 4 to print the Confusion Matrix and all the rates for both the Black and Caucasian Group according to this new predictor. Additionally, plot the ROC curves for the entire group as well as for the `African-American` and `Caucasian` group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 5**\n",
    "Assume that the variable `y_pred` holds the prediction of whether or not an individual recidivated according to the logistic regression model based on the 7 variables in this lab. Recall that this corresponds to using a (default) threshold of 0.5 for the prediction **probability**. Let `y_pred_prob` hold the prediction probabilities of each individual recidivating according to our model, which can be interpreted as the score received by each individual (on a scale of 0 to 1).\n",
    "We'd like to examine the separation criteria for this model. To examine this, plot the distribution of scores received by the positive class (recidivists) and the distribution of scores received by the negative class (non-recidivists) for black defendants and for white defendants. Round up each score to the closest multiple of 0.1 so that the scores range from 0.1 to 1.0.\n",
    "Before you start with this task, make sure you revisit the separation criteria for a non-binary classifier:\n",
    "$$ P \\lbrace R = r\\lvert Y=1,A=a \\rbrace= P \\lbrace R=r \\lvert Y=1,A=a\\rbrace$$\n",
    "$$P \\lbrace R=r \\lvert Y=0,A=b \\rbrace =P\\lbrace R =r\\lvert Y=0,A=b\\rbrace $$\n",
    "for all scores $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
